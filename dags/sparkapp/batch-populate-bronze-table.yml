apiVersion: "sparkoperator.k8s.io/v1beta2"
kind: SparkApplication
metadata:
  name: populate-table
  namespace: airflow
spec:

  type: Java
  mode: cluster
  image: "ddcj/spark-job:omea-pocv10"
  mainClass: LoadToBronzeTables
  mainApplicationFile: "local:///opt/spark/apps/app.jar"
  arguments:
    - "s3a://datalake/voice/"
    - "voice"
    - "bronze.voice"

  sparkVersion: "4.0.1"
  restartPolicy:
    type: Never

  sparkConf:

    "spark.eventLog.enabled": "true"
    "spark.eventLog.dir": "s3a://spark-logs/"
    
    "spark.sql.extensions": "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions"
    "spark.sql.catalog.iceberg": "org.apache.iceberg.spark.SparkCatalog"
    "spark.sql.catalog.iceberg.type": "rest"
    "spark.sql.catalog.iceberg.uri": "http://iceberg-rest.lakehouse.svc.cluster.local:8181"
    "spark.sql.catalog.iceberg.io-impl": "org.apache.iceberg.aws.s3.S3FileIO"
    "spark.sql.catalog.iceberg.s3.endpoint": "http://minio.lakehouse.svc.cluster.local:9000"
    "spark.sql.catalog.iceberg.warehouse": "s3://lakehouse/"
    "spark.sql.catalog.iceberg.s3.region": "us-east-1"
    "spark.sql.catalog.iceberg.s3.path-style-access": "true"
    "spark.sql.defaultCatalog": "iceberg"

    # Adding MinIO credentials directly for Iceberg REST catalog access
    "spark.sql.catalog.iceberg.s3.access-key-id": "admin"
    "spark.sql.catalog.iceberg.s3.secret-access-key": "password"
    
    # Also, ensure the signer is compatible with MinIO (which usually uses V4)
    "spark.sql.catalog.iceberg.s3.signer-type": "standard"

    "spark.hadoop.fs.s3a.endpoint": "http://minio.lakehouse.svc.cluster.local:9000"
    "spark.hadoop.fs.s3a.path.style.access": "true"
    "spark.hadoop.fs.s3a.access.key": "admin"
    "spark.hadoop.fs.s3a.secret.key": "password"
    "spark.hadoop.fs.s3a.impl": "org.apache.hadoop.fs.s3a.S3AFileSystem"

  driver:
    cores: 1
    memory: "512m"
    serviceAccount: spark-operator-spark
    # Injecting Secret as Env Vars
    envFrom:
      - secretRef:
          name: minio-creds
    # Mapping Env Vars to Spark Properties
    env:
      - name: AWS_ACCESS_KEY_ID
        valueFrom:
          secretKeyRef:
            name: minio-creds
            key: AWS_ACCESS_KEY_ID
      - name: AWS_SECRET_ACCESS_KEY
        valueFrom:
          secretKeyRef:
            name: minio-creds
            key: AWS_REGION
      - name: AWS_REGION
        valueFrom:
          secretKeyRef:
            name: minio-creds
            key: AWS_REGION

  executor:
    cores: 1
    instances: 1
    memory: "512m"
    # Executors need the credentials too!
    env:
      - name: AWS_ACCESS_KEY_ID
        valueFrom:
          secretKeyRef:
            name: minio-creds
            key: AWS_ACCESS_KEY_ID 
      - name: AWS_SECRET_ACCESS_KEY
        valueFrom:
          secretKeyRef:
            name: minio-creds
            key: AWS_SECRET_ACCESS_KEY
      - name: AWS_REGION
        valueFrom:
          secretKeyRef:
            name: minio-creds
            key: AWS_REGION 